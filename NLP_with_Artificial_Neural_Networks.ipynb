{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks and Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# select categories and load the training and test data\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med', 'sci.space',\n",
    "              'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "# load training and test data\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and download NLTK (natural language toolkit for more expanded libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to vectors of word counts\n",
    "\n",
    "# import and use CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#count_vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_test_counts = count_vect.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate label vectors for training and multiclass ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(twenty_train.target)\n",
    "train_vTarget=lb.transform(twenty_train.target)\n",
    "test_vTarget=lb.transform(twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 2, ..., 1, 7, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ANN (discuss parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "10\n",
      "[0 0 0 0 1 0 0 0 0 0]\n",
      "-----------\n",
      "[0 0 0 0 1 0 0 0 0 0]\n",
      "0.8526831785345718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf.fit(X_train_counts, train_vTarget)\n",
    "print(clf.classes_)\n",
    "print(clf.n_outputs_)\n",
    "\n",
    "# make predictions on test data\n",
    "clf.out_activation_ = 'softmax'\n",
    "predicted = clf.predict(X_test_counts)\n",
    "\n",
    "print(predicted[1])\n",
    "print('-----------')\n",
    "print(test_vTarget[1])\n",
    "predicted = lb.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526831785345718\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.38      0.92      0.54       319\n",
      "         comp.graphics       0.94      0.84      0.89       389\n",
      "          misc.forsale       0.94      0.88      0.91       390\n",
      "             rec.autos       0.97      0.82      0.89       396\n",
      "       rec.motorcycles       1.00      0.89      0.94       398\n",
      "    rec.sport.baseball       0.98      0.86      0.92       397\n",
      "      rec.sport.hockey       0.99      0.95      0.97       399\n",
      "               sci.med       0.97      0.67      0.79       396\n",
      "             sci.space       1.00      0.80      0.89       394\n",
      "soc.religion.christian       0.95      0.91      0.93       398\n",
      "\n",
      "              accuracy                           0.85      3876\n",
      "             macro avg       0.91      0.85      0.87      3876\n",
      "          weighted avg       0.92      0.85      0.87      3876\n",
      "\n",
      "[[293   1   0   0   0   1   0   5   0  19]\n",
      " [ 59 325   3   0   0   1   0   0   1   0]\n",
      " [ 40   3 343   4   0   0   0   0   0   0]\n",
      " [ 62   2   5 326   0   0   0   1   0   0]\n",
      " [ 35   0   4   4 355   0   0   0   0   0]\n",
      " [ 49   0   3   0   0 342   3   0   0   0]\n",
      " [ 19   0   0   0   0   2 378   0   0   0]\n",
      " [120   3   3   2   0   1   0 267   0   0]\n",
      " [ 66   9   1   1   0   0   0   3 314   0]\n",
      " [ 32   2   1   0   0   1   0   0   0 362]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045407636738906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(25,), random_state=1)\n",
    "clf.fit(X_train_counts, train_vTarget)\n",
    "\n",
    "# make predictions on test data\n",
    "clf.out_activation_ = 'softmax'\n",
    "predicted = clf.predict(X_test_counts)\n",
    "\n",
    "predicted = lb.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045407636738906\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.56      0.88      0.69       319\n",
      "         comp.graphics       0.93      0.88      0.90       389\n",
      "          misc.forsale       0.92      0.93      0.93       390\n",
      "             rec.autos       0.96      0.89      0.92       396\n",
      "       rec.motorcycles       1.00      0.93      0.96       398\n",
      "    rec.sport.baseball       0.97      0.93      0.95       397\n",
      "      rec.sport.hockey       0.97      0.97      0.97       399\n",
      "               sci.med       0.96      0.81      0.88       396\n",
      "             sci.space       0.97      0.89      0.93       394\n",
      "soc.religion.christian       0.92      0.94      0.93       398\n",
      "\n",
      "              accuracy                           0.90      3876\n",
      "             macro avg       0.92      0.90      0.91      3876\n",
      "          weighted avg       0.92      0.90      0.91      3876\n",
      "\n",
      "[[280   1   0   0   0   1   0   7   3  27]\n",
      " [ 27 341   8   3   0   4   2   0   4   0]\n",
      " [ 19   3 362   5   0   0   1   0   0   0]\n",
      " [ 31   2   8 352   1   0   0   1   1   0]\n",
      " [ 16   0   4   7 371   0   0   0   0   0]\n",
      " [ 17   0   3   0   0 368   8   0   1   0]\n",
      " [  5   0   0   0   0   6 387   0   0   1]\n",
      " [ 64   6   3   0   0   1   1 319   0   2]\n",
      " [ 24  12   3   0   0   0   0   4 350   1]\n",
      " [ 15   2   1   0   0   1   0   1   2 376]]\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_sw = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_sw_counts = count_vect_sw.fit_transform(twenty_train.data)\n",
    "X_test_sw_counts = count_vect_sw.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847265221878225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf.fit(X_train_sw_counts, train_vTarget)\n",
    "\n",
    "# make predictions on test data\n",
    "clf.out_activation_ = 'softmax'\n",
    "predicted = clf.predict(X_test_sw_counts)\n",
    "\n",
    "predicted = lb.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847265221878225\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.36      0.91      0.52       319\n",
      "         comp.graphics       0.96      0.79      0.87       389\n",
      "          misc.forsale       0.97      0.83      0.89       390\n",
      "             rec.autos       0.98      0.82      0.89       396\n",
      "       rec.motorcycles       0.99      0.92      0.95       398\n",
      "    rec.sport.baseball       0.98      0.86      0.92       397\n",
      "      rec.sport.hockey       0.99      0.91      0.95       399\n",
      "               sci.med       0.98      0.70      0.82       396\n",
      "             sci.space       0.98      0.81      0.89       394\n",
      "soc.religion.christian       0.94      0.93      0.93       398\n",
      "\n",
      "              accuracy                           0.85      3876\n",
      "             macro avg       0.91      0.85      0.86      3876\n",
      "          weighted avg       0.92      0.85      0.87      3876\n",
      "\n",
      "[[290   1   0   0   0   1   0   4   1  22]\n",
      " [ 75 309   1   0   0   1   0   0   2   1]\n",
      " [ 59   1 324   5   0   1   0   0   0   0]\n",
      " [ 63   1   5 323   2   0   0   1   1   0]\n",
      " [ 28   0   2   3 365   0   0   0   0   0]\n",
      " [ 52   0   1   0   0 341   2   0   1   0]\n",
      " [ 31   0   0   0   0   2 365   0   1   0]\n",
      " [113   1   1   0   0   1   1 279   0   0]\n",
      " [ 66   7   1   0   0   0   0   1 319   0]\n",
      " [ 27   1   0   0   0   1   0   0   0 369]]\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "#stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "stemmed_count_vect = StemmedCountVectorizer()\n",
    "X_train_stem_counts = stemmed_count_vect.fit_transform(twenty_train.data)\n",
    "X_test_stem_counts = stemmed_count_vect.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8606811145510835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf.fit(X_train_stem_counts, train_vTarget)\n",
    "\n",
    "# make predictions on test data\n",
    "clf.out_activation_ = 'softmax'\n",
    "predicted = clf.predict(X_test_stem_counts)\n",
    "\n",
    "predicted = lb.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8606811145510835\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.40      0.91      0.55       319\n",
      "         comp.graphics       0.94      0.84      0.89       389\n",
      "          misc.forsale       0.94      0.86      0.90       390\n",
      "             rec.autos       0.97      0.86      0.91       396\n",
      "       rec.motorcycles       1.00      0.89      0.94       398\n",
      "    rec.sport.baseball       0.99      0.87      0.93       397\n",
      "      rec.sport.hockey       0.99      0.92      0.95       399\n",
      "               sci.med       0.97      0.73      0.83       396\n",
      "             sci.space       0.98      0.83      0.90       394\n",
      "soc.religion.christian       0.95      0.90      0.92       398\n",
      "\n",
      "              accuracy                           0.86      3876\n",
      "             macro avg       0.91      0.86      0.87      3876\n",
      "          weighted avg       0.92      0.86      0.88      3876\n",
      "\n",
      "[[291   0   0   0   0   1   0   5   3  19]\n",
      " [ 50 328   6   0   0   0   1   0   3   1]\n",
      " [ 48   3 335   4   0   0   0   0   0   0]\n",
      " [ 47   1   6 340   0   0   0   1   1   0]\n",
      " [ 36   0   3   5 354   0   0   0   0   0]\n",
      " [ 47   0   0   0   0 347   3   0   0   0]\n",
      " [ 28   0   0   0   0   3 368   0   0   0]\n",
      " [100   4   4   0   0   0   0 288   0   0]\n",
      " [ 54  11   1   0   0   0   0   2 326   0]\n",
      " [ 34   1   1   0   0   1   0   1   1 359]]\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data to a TF-IDF representation (Note change to max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# clf_2 = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8771929824561403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(activation='logistic', solver='adam', max_iter=100, alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf.fit(X_train_tfidf, train_vTarget)\n",
    "\n",
    "# make predictions on test data\n",
    "clf.out_activation_ = 'softmax'\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "predicted = lb.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8771929824561403\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.43      0.90      0.58       319\n",
      "         comp.graphics       0.95      0.86      0.90       389\n",
      "          misc.forsale       0.96      0.89      0.92       390\n",
      "             rec.autos       0.97      0.86      0.91       396\n",
      "       rec.motorcycles       0.99      0.89      0.94       398\n",
      "    rec.sport.baseball       0.98      0.91      0.94       397\n",
      "      rec.sport.hockey       0.98      0.96      0.97       399\n",
      "               sci.med       0.97      0.73      0.83       396\n",
      "             sci.space       0.99      0.85      0.91       394\n",
      "soc.religion.christian       0.94      0.93      0.94       398\n",
      "\n",
      "              accuracy                           0.88      3876\n",
      "             macro avg       0.92      0.88      0.89      3876\n",
      "          weighted avg       0.93      0.88      0.89      3876\n",
      "\n",
      "[[287   0   0   0   1   1   0   4   2  24]\n",
      " [ 52 334   2   0   0   1   0   0   0   0]\n",
      " [ 33   3 348   4   1   0   1   0   0   0]\n",
      " [ 47   1   6 339   1   0   0   1   1   0]\n",
      " [ 36   0   3   5 354   0   0   0   0   0]\n",
      " [ 31   0   1   0   0 360   5   0   0   0]\n",
      " [  9   0   0   0   0   4 384   0   1   1]\n",
      " [ 97   5   4   0   0   1   0 289   0   0]\n",
      " [ 50   8   0   0   0   0   0   3 333   0]\n",
      " [ 22   2   0   0   0   1   0   0   1 372]]\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "print (np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the search for a good ANN model on just the comp.* subset of newsgroups\n",
    "\n",
    "### Make the following choices sequentially (1) hidden layer sizes, (2) include or ignore stopwords, (3) count vectors vs tfidf vectors, and then (4) stemming or not. I suggest using max_iter of at least 100 (default is 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 comp.sys.ibm.pc.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "3 comp.sys.mac.hardware\n",
      "3 comp.sys.mac.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "4 comp.windows.x\n",
      "3 comp.sys.mac.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "1 comp.os.ms-windows.misc\n",
      "4 comp.windows.x\n",
      "4 comp.windows.x\n",
      "1 comp.os.ms-windows.misc\n",
      "1 comp.os.ms-windows.misc\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "3 comp.sys.mac.hardware\n",
      "1 comp.os.ms-windows.misc\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "2 comp.sys.ibm.pc.hardware\n",
      "4 comp.windows.x\n",
      "\n",
      " [\"From: lemons@cadsys.enet.dec.com\\nSubject: Xremote into X11R6?\\nReply-To: lemons@cadsys.enet.dec.com ()\\nOrganization: Digital Equipment Corporation\\nLines: 12\\nX-Newsreader: mxrn 6.18\\n\\n\\nHi!\\n\\nI remember reading (or hallucinating) that NCD's PC-Xremote functionality had \\nbeen given, by NCD, to MIT for inclusion in X11R6.  Is this true?  If so,\\n(set mode/cheap) can I just wait for X11R6 to get compressed serial line\\nX server support?\\n\\nThanks!\\n\\nTerry Lemons\\nDigital Equipment Corporation\\n\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cats = ['comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x']\n",
    "\n",
    "our_train = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
    "our_train.target[:25]\n",
    "for p in our_train.target[:25]:\n",
    "    target_id = our_train.target[p]\n",
    "    print(target_id, our_train.target_names[target_id])\n",
    "\n",
    "# print the first article\n",
    "print(\"\\n\", our_train.data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ct_vec = CountVectorizer(stop_words='english')\n",
    "X_Train_counts = ct_vec.fit_transform(our_train.data)\n",
    "\n",
    "ct_vec.vocabulary_.get(u'algorithm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [\"From: lonewolf@muse.Corp.Sun.COM (Peter Pak)\\nSubject: Re: 386 Motherboard advice needed\\nOrganization: Sun Microsystems\\nLines: 12\\nDistribution: world\\nReply-To: lonewolf@muse.Corp.Sun.COM\\nNNTP-Posting-Host: muse.corp.sun.com\\n\\nMaybe I should have been clearer.  I have a Intel 386DX/25 that I would\\nlike to use to put together a system however all the motherboards that\\nthe local vendors are now selling are running either at 33 or 40 MHz.  I\\nguess I can cross my fingers and hope the CPU runs at that speed. ;^)\\n\\nI think I'll take Mark's advice and see if any of the boards have\\na socketed oscillator and head down to the local electronics store...\\n\\nThanks for the info...\\n\\n=B^)\\n\\n\"]\n"
     ]
    }
   ],
   "source": [
    "our_test = fetch_20newsgroups(subset='test', categories=cats, shuffle=True, random_state=42)\n",
    "\n",
    "# note that I am using .transform instead of .fit_transform. this keeps the columns the same as the training set\n",
    "X_Test_counts = ct_vec.transform(our_test.data)\n",
    "\n",
    "print(\"\\n\", our_test.data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb_1 = preprocessing.LabelBinarizer()\n",
    "lb_1.fit(our_train.target)\n",
    "train_vTarget_1=lb_1.transform(our_train.target)\n",
    "test_vTarget_1=lb_1.transform(our_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTING BEST HIDDEN LAYER SIZE USING GRIDSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# finish the function below\n",
    "gs_params = {\"hidden_layer_sizes\": [110,125,150]  \n",
    "             }\n",
    "est = MLPClassifier()\n",
    "gs_clf = GridSearchCV(estimator=est, param_grid=gs_params)\n",
    "gs_results = gs_clf.fit(X_Train_counts, our_train.target)\n",
    "\n",
    "hl_size = gs_results.best_params_\n",
    "print(hl_size)\n",
    "# print(gs_results.best_params_)\n",
    "\n",
    "print(gs_results.best_estimator_)\n",
    "\n",
    "best_clf = gs_results.best_estimator_\n",
    "gs_predicted = gs_clf.predict(X_Test_counts)\n",
    "\n",
    "print (np.mean(gs_predicted == our_test.target))\n",
    "\n",
    "\n",
    "print(metrics.classification_report(our_test.target, gs_predicted,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "\n",
    "print(metrics.confusion_matrix(our_test.target, gs_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING COUNT VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_3 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_3.fit(X_Train_counts, train_vTarget_1)\n",
    "print(clf_3.classes_)\n",
    "print(clf_3.n_outputs_)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_3.out_activation_ = 'softmax'\n",
    "predicted_3 = clf_3.predict(X_Test_counts)\n",
    "\n",
    "print(predicted_3[1])\n",
    "print('-----------')\n",
    "print(test_vTarget_1[1])\n",
    "predicted_3 = lb_1.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_3 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_3,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_Train_tfidf = tfidf_transformer.fit_transform(X_Train_counts)\n",
    "\n",
    "# clf_2 = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "X_Test_tfidf = tfidf_transformer.transform(X_Test_counts)\n",
    "\n",
    "clf_4 = MLPClassifier(activation='logistic', solver='adam', max_iter=100, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf.fit(X_Train_tfidf, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_4.out_activation_ = 'softmax'\n",
    "predicted_4 = clf.predict(X_Test_tfidf)\n",
    "\n",
    "predicted_4 = lb_1.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_4 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_4,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_vect_sw = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_Train_sw_counts = ct_vect_sw.fit_transform(our_train.data)\n",
    "X_Test_sw_counts = ct_vect_sw.transform(our_test.data)\n",
    "\n",
    "clf_5 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_5.fit(X_Train_sw_counts, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_5.out_activation_ = 'softmax'\n",
    "predicted_5 = clf_5.predict(X_Test_sw_counts)\n",
    "\n",
    "predicted_5 = lb_1.inverse_transform(predicted_5)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_5 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_5,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "#stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "stemmed_ct_vect = StemmedCountVectorizer()\n",
    "X_Train_stem_counts = stemmed_ct_vect.fit_transform(our_train.data)\n",
    "X_Test_stem_counts = stemmed_ct_vect.transform(our_test.data)\n",
    "\n",
    "\n",
    "clf_6 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_6.fit(X_Train_stem_counts, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_6.out_activation_ = 'softmax'\n",
    "predicted_6 = clf_6.predict(X_Test_stem_counts)\n",
    "\n",
    "predicted_6 = lb_1.inverse_transform(predicted_6)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_6 == our_test.target)) \n",
    "\n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_6,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE SINGLE PIECE OF CODE FOR ONE-CLICK RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "cats = ['comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x']\n",
    "\n",
    "our_train = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
    "our_train.target[:25]\n",
    "for p in our_train.target[:25]:\n",
    "    target_id = our_train.target[p]\n",
    "    print(target_id, our_train.target_names[target_id])\n",
    "\n",
    "# print the first article\n",
    "print(\"\\n\", our_train.data[:1])\n",
    "\n",
    "ct_vec = CountVectorizer(stop_words='english')\n",
    "X_Train_counts = ct_vec.fit_transform(our_train.data)\n",
    "\n",
    "ct_vec.vocabulary_.get(u'algorithm')\n",
    "\n",
    "our_test = fetch_20newsgroups(subset='test', categories=cats, shuffle=True, random_state=42)\n",
    "\n",
    "# note that I am using .transform instead of .fit_transform. this keeps the columns the same as the training set\n",
    "X_Test_counts = ct_vec.transform(our_test.data)\n",
    "\n",
    "print(\"\\n\", our_test.data[:1])\n",
    "\n",
    "lb_1 = preprocessing.LabelBinarizer()\n",
    "lb_1.fit(our_train.target)\n",
    "train_vTarget_1=lb_1.transform(our_train.target)\n",
    "test_vTarget_1=lb_1.transform(our_test.target)\n",
    "\n",
    "# finish the function below\n",
    "gs_params = {\"hidden_layer_sizes\": [110,125,150]  \n",
    "             }\n",
    "est = MLPClassifier()\n",
    "gs_clf = GridSearchCV(estimator=est, param_grid=gs_params)\n",
    "gs_results = gs_clf.fit(X_Train_counts, our_train.target)\n",
    "\n",
    "hl_size = gs_results.best_params_\n",
    "print(hl_size)\n",
    "# print(gs_results.best_params_)\n",
    "\n",
    "print(gs_results.best_estimator_)\n",
    "\n",
    "best_clf = gs_results.best_estimator_\n",
    "gs_predicted = gs_clf.predict(X_Test_counts)\n",
    "\n",
    "print (np.mean(gs_predicted == our_test.target))\n",
    "\n",
    "\n",
    "print(metrics.classification_report(our_test.target, gs_predicted,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "\n",
    "print(metrics.confusion_matrix(our_test.target, gs_predicted))\n",
    "\n",
    "clf_3 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_3.fit(X_Train_counts, train_vTarget_1)\n",
    "print(clf_3.classes_)\n",
    "print(clf_3.n_outputs_)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_3.out_activation_ = 'softmax'\n",
    "predicted_3 = clf_3.predict(X_Test_counts)\n",
    "\n",
    "print(predicted_3[1])\n",
    "print('-----------')\n",
    "print(test_vTarget_1[1])\n",
    "predicted_3 = lb_1.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_3 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_3,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_3))\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_Train_tfidf = tfidf_transformer.fit_transform(X_Train_counts)\n",
    "\n",
    "# clf_2 = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "X_Test_tfidf = tfidf_transformer.transform(X_Test_counts)\n",
    "\n",
    "clf_4 = MLPClassifier(activation='logistic', solver='adam', max_iter=100, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf.fit(X_Train_tfidf, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_4.out_activation_ = 'softmax'\n",
    "predicted_4 = clf.predict(X_Test_tfidf)\n",
    "\n",
    "predicted_4 = lb_1.inverse_transform(predicted)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_4 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_4,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_4))\n",
    "\n",
    "ct_vect_sw = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_Train_sw_counts = ct_vect_sw.fit_transform(our_train.data)\n",
    "X_Test_sw_counts = ct_vect_sw.transform(our_test.data)\n",
    "\n",
    "clf_5 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_5.fit(X_Train_sw_counts, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_5.out_activation_ = 'softmax'\n",
    "predicted_5 = clf_5.predict(X_Test_sw_counts)\n",
    "\n",
    "predicted_5 = lb_1.inverse_transform(predicted_5)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_5 == our_test.target)) \n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_5,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_5))\n",
    "\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "#stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "stemmed_ct_vect = StemmedCountVectorizer()\n",
    "X_Train_stem_counts = stemmed_ct_vect.fit_transform(our_train.data)\n",
    "X_Test_stem_counts = stemmed_ct_vect.transform(our_test.data)\n",
    "\n",
    "\n",
    "clf_6 = MLPClassifier(activation='logistic', solver='adam', max_iter=50, alpha=1e-5, hidden_layer_sizes=(hl_size,), random_state=1)\n",
    "clf_6.fit(X_Train_stem_counts, train_vTarget_1)\n",
    "\n",
    "# make predictions on test data\n",
    "clf_6.out_activation_ = 'softmax'\n",
    "predicted_6 = clf_6.predict(X_Test_stem_counts)\n",
    "\n",
    "predicted_6 = lb_1.inverse_transform(predicted_6)\n",
    "\n",
    "# print accuracy\n",
    "print (np.mean(predicted_6 == our_test.target)) \n",
    "\n",
    "\n",
    "# print precision and recall statistics\n",
    "print(metrics.classification_report(our_test.target, predicted_6,\n",
    "    target_names=our_test.target_names))\n",
    "\n",
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(our_test.target, predicted_6))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88de8a03d390c9821a705f8812eeaeda47efe29e530260f109fd5a47346b85c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
